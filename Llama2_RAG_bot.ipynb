{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160a259c-1958-4a98-bfdf-dec85ad78a3f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will pick up where we have left in the [preparation notebook](https://github.com/shaaagri/iat481-nlp-proj/blob/main/LLama2_vanilla_bot.ipynb) and will add a vector store with a retriever to the pipeline. This should be enough to lay the framework to realize our intention - a chatbot powered by RAG (Retrieval Augmented Generation), which is, in essence, a special case of automated prompt engineering. Just a reminder, the specialized knowledge we plan to inject into the chatbot is concerned with sleep hygiene and related science-backed tips.\n",
    "\n",
    "**Note**: this notebook is intended to be run locally in Jupyter and was never tested in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d7aec-5f28-426a-94df-65738a8e4575",
   "metadata": {},
   "source": [
    "![title](images/RAG_overview_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103978e6-c871-43b6-8f08-ea7833422600",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "1. Setting Up LLama-2 and LangChain\n",
    "2. Text Embeddings and the Vector Store\n",
    "3. Preparing a RAG Pipeline Using Sample Data\n",
    "4. Completing the RAG Pipeline Using Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebea9d-72bf-4cee-8df1-fd6e74082d60",
   "metadata": {},
   "source": [
    "# Setting Up LLama-2 and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f27209-1ade-4bd0-a320-9914a16ed75f",
   "metadata": {},
   "source": [
    "The next section mostly repeats the code from the preparation notebook. If that notebook has been run already, running this section may not be required as the kernel should keep its state. However, this section may diverge from the previous notebook, so it is recommended to re-run all of the cells here.\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae9f89b-742a-4b9f-89b7-53386f2493b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
      "env: FORCE_CMAKE=1\n",
      "Using pip 24.0 from C:\\Program Files\\Python312\\Lib\\site-packages\\pip (python 3.12)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (0.2.61)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\program files\\python312\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python312\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python312\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\program files\\python312\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (0.2.61)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\program files\\python312\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python312\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (0.1.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\program files\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.0.32)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.1.41)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\program files\\python312\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\program files\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.2.0,>=0.1.41->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "%set_env CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
    "%set_env FORCE_CMAKE=1\n",
    "!pip install llama-cpp-python --upgrade --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140b2154-7a77-45e7-918e-88f5424adb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4bb1-a3ee-4bc2-8599-ace8beaaffd9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce84bf58-d0fb-453e-b823-7547ba3df335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-7B-chat-GGUF\"\n",
    "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bae935-e96d-452c-a46c-61f46bd397ff",
   "metadata": {},
   "source": [
    "The following line will download the model, but first it will check Hugging Face Hub's cache folder where it may have been saved after previous notebook runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e652df86-3a4b-4063-84fb-b6f36422dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b056cd-275c-428f-9704-215dad810ca2",
   "metadata": {},
   "source": [
    "### LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b95bf8-df66-4200-ac1b-6881d05b1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3ffa2-73a3-49bb-b1a3-cccbec6830db",
   "metadata": {},
   "source": [
    "Here we write the system prompt inside a basic template used to initialize the chatbot. During our experements we have noticed it exerts a lot of influence on the bot's behavior, being no less important than the Llama-2 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ab37b1-58ea-4a6f-a112-b07ebbe0fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template='''[INST]\n",
    "<<SYS>>\n",
    "You are helpful, respectful, caring and honest assistant. You do not have expressions or emotions. You are objective and provide everything that is helpful to know given the question, but you are not chatty. Answer as helpfully as you possibly can.\n",
    "<</SYS>>\n",
    "\n",
    "USER: {question}\n",
    "\n",
    "ASSISTANT: \n",
    "[/INST]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb24f6d6-f121-4947-b314-28c935e4d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea5859-c6e4-4db9-9941-1d295e2d3983",
   "metadata": {},
   "source": [
    "The model then can be easily initialized thanks to LangChain's built-in `llama.cpp` wrapper ([documentation](https://python.langchain.com/docs/integrations/llms/llamacpp/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb4258a-a809-4a8f-8783-6b0495474941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eacb6b6c-e07e-49ef-8e0b-2fa8c705e3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\Narratic-DEV002\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-chat-GGUF\\snapshots\\191239b3e26b2882fb562ffccdd1cf0f65402adb\\llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    # Make sure the model path is correct for your system!\n",
    "    model_path=model_path,\n",
    "    \n",
    "    temperature=0.6,\n",
    "    n_gpu_layers=-1,  # -1 stands for offloading all layers of the model to GPU => better performance (we've got enough VRAM)\n",
    "    n_ctx=4096,  # IMPORTANT for RAG, the default for quantized GGUF models is only 512\n",
    "    max_tokens=1024,\n",
    "    repeat_penalty=1.02,\n",
    "    top_p=0.8, # nucleus sampling\n",
    "    top_k=150,  # sample from k top tokens \n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694af692-a1c8-4587-bcdf-7f04dfce65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "# debugging on demand\n",
    "set_debug(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82921663-0f27-42dc-9a20-d938dfc48ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question='Describe the main campus of the Simon Fraser University'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2941e220-15f7-4dda-809d-c03f22b39aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"[INST]\\n<<SYS>>\\nYou are helpful, respectful, caring and honest assistant. You do not have expressions or emotions. You are objective and provide everything that is helpful to know given the question, but you are not chatty. Answer as helpfully as you possibly can.\\n<</SYS>>\\n\\nUSER: Describe the main campus of the Simon Fraser University\\n\\nASSISTANT: \\n[/INST]\"\n",
      "  ]\n",
      "}\n",
      "Thank you for asking! The main campus of Simon Fraser University is located in Burnaby, British Columbia, Canada. It spans over 80 acres and is home to many of the university's academic and administrative buildings, as well as student residences and recreational facilities.\n",
      "The main campus is situated near the intersection of University Boulevard and Central Boulevard, and is easily accessible by public transportation or car.\n",
      "Some of the notable buildings on the main campus include:\n",
      "* The SFU Library, which houses a vast collection of books, journals, and other resources for students and researchers.\n",
      "* The Academic Services Building, which provides support services for students, including academic advising, career counseling, and language support.\n",
      "* The Student Union Building, which offers a variety of amenities and services, including food services, a bookstore, and event spaces.\n",
      "* The SFU Theatre, which hosts a variety of performances and events throughout the year, including concerts, plays, and dance performances.\n",
      "* The Athletics and Recreation Centre, which features state-of-the-art fitness facilities, sports fields, and recreational spaces for students and members of the broader community.\n",
      "Overall, the main campus of Simon Fraser University is a vibrant and dynamic space that offers a wide range of academic and recreational opportunities for students and visitors alike."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4349.07 ms\n",
      "llama_print_timings:      sample time =      64.86 ms /   297 runs   (    0.22 ms per token,  4578.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11367.41 ms /    98 tokens (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:        eval time =   31528.94 ms /   296 runs   (  106.52 ms per token,     9.39 tokens per second)\n",
      "llama_print_timings:       total time =   44204.20 ms /   394 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:LlamaCpp] [44.21s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thank you for asking! The main campus of Simon Fraser University is located in Burnaby, British Columbia, Canada. It spans over 80 acres and is home to many of the university's academic and administrative buildings, as well as student residences and recreational facilities.\\nThe main campus is situated near the intersection of University Boulevard and Central Boulevard, and is easily accessible by public transportation or car.\\nSome of the notable buildings on the main campus include:\\n* The SFU Library, which houses a vast collection of books, journals, and other resources for students and researchers.\\n* The Academic Services Building, which provides support services for students, including academic advising, career counseling, and language support.\\n* The Student Union Building, which offers a variety of amenities and services, including food services, a bookstore, and event spaces.\\n* The SFU Theatre, which hosts a variety of performances and events throughout the year, including concerts, plays, and dance performances.\\n* The Athletics and Recreation Centre, which features state-of-the-art fitness facilities, sports fields, and recreational spaces for students and members of the broader community.\\nOverall, the main campus of Simon Fraser University is a vibrant and dynamic space that offers a wide range of academic and recreational opportunities for students and visitors alike.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Thank you for asking! The main campus of Simon Fraser University is located in Burnaby, British Columbia, Canada. It spans over 80 acres and is home to many of the university's academic and administrative buildings, as well as student residences and recreational facilities.\\nThe main campus is situated near the intersection of University Boulevard and Central Boulevard, and is easily accessible by public transportation or car.\\nSome of the notable buildings on the main campus include:\\n* The SFU Library, which houses a vast collection of books, journals, and other resources for students and researchers.\\n* The Academic Services Building, which provides support services for students, including academic advising, career counseling, and language support.\\n* The Student Union Building, which offers a variety of amenities and services, including food services, a bookstore, and event spaces.\\n* The SFU Theatre, which hosts a variety of performances and events throughout the year, including concerts, plays, and dance performances.\\n* The Athletics and Recreation Centre, which features state-of-the-art fitness facilities, sports fields, and recreational spaces for students and members of the broader community.\\nOverall, the main campus of Simon Fraser University is a vibrant and dynamic space that offers a wide range of academic and recreational opportunities for students and visitors alike.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.format(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb91bf9-114e-4372-b3b1-c8afaabde07c",
   "metadata": {},
   "source": [
    "# Text Embeddings and the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cebe7-10b4-4e57-bd5d-8a9bf64380b5",
   "metadata": {},
   "source": [
    "As our RAG bot is going to rely on the supply of extra knowledge that we will manually package into the project (in the form of Q&A data), here comes a crucial part - choosing a text embedding model and the vector store. The former will take care of converting our textual Q&A data into vector representation which is required to do the semantic similarity comparison later - in other words, to match to the best of our ability the user question to the appropriate piece of information within the extra knowledge. The latter is going to neatly store these representations, providing access to them as needed. These two nodes are cornerstones of any RAG project and the use cases and the range of choices for the models and the vector stores are well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c849a2d-bc69-4593-800e-5ddbc0dcfbae",
   "metadata": {},
   "source": [
    "### Choosing the Text Embedding Model\n",
    "\n",
    "For a long time, there was little choice for a specific model that produces the embeddings beside OpenAI's `ada-002`, which is provided through API requiring a small fee to use. However, by April 2024 (the time of writing this notebook) the range has considerably increased, and now there are not only players in the market (e.g. [Cohere](https://cohere.com/embeddings), [Jina](https://jina.ai/embeddings/) - both offer a free tier) but also open-source text embeddings model can be found, such as `SentenceTransformers` available at Hugging Face ([link](https://huggingface.co/sentence-transformers)). \n",
    "\n",
    "As students we are delighted to be able to use another model free of charge; our only question is whether it performs comparably to ada-002. The good news is that our brief research has told us we should be fine with the open-source Sentence Transformers (which come as a [family of models](https://www.sbert.net/docs/pretrained_models.html]) each trading off performance for quality in various ways) - here are the resources we are referring to: [(1)](https://iamnotarobot.substack.com/p/should-you-use-openais-embeddings), [(2)](https://www.reddit.com/r/MachineLearning/comments/11okrni/discussion_compare_openai_and_sentencetransformer/), [(3)](https://supabase.com/blog/fewer-dimensions-are-better-pgvector), ([4](https://weaviate.io/blog/how-to-choose-a-sentence-transformer-from-hugging-face])).\n",
    "\n",
    "The consensus seems to be that it's not necessary to use ada-002 at all as the open-source models match it and sometimes even exceed it in performance. One particular text embedding model that seems to have an ideal balance between size, speed, and accuracy is `all-MiniLM-L6-v2`. It also has an \"older brother\", a slightly larger model `all-MiniLM-L12-v2`, and according to [this table](https://www.sbert.net/docs/pretrained_models.html), it's only marginally better than all-MiniLM-L6-v2, while being significantly slower. All in all, we think the all-MiniLM-L6-v2 model is an excellent start, given our use case is mostly concerned with general purpose English words. It is also supported by LangChain out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbdd4b2-b569-4133-990c-c900606ded4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.39.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python312\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\program files\\python312\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7594c22d-af8a-486b-b279-c17070b935e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893d720-db6e-47a8-b555-fbeba5d191c3",
   "metadata": {},
   "source": [
    "### Choosing the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd32a8-c379-4bb4-a95f-da29945ded1d",
   "metadata": {},
   "source": [
    "As for the vector database that is going to store the embeddings, the decision is considerably easier. It comes down to two well-known alternatives: `Pinecone` (managed) and `ChromaDB` (self-hosted). To remind the reader, our guiding design principle is to get away with open-source and/or free-tier components for 100% of the pipeline, hence ChromeDB is the obvious choice. To consult with some literature we checked, for instance, [this article](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df), and it confirmed our assumptions that ChromaDB should be more than enough for what is just a student prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ba1519-b873-4a1f-9467-6a9c32bcc73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chromadb in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (0.4.24)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\program files\\python312\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (2.6.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.110.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.24.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\program files\\python312\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\program files\\python312\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (3.10.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\program files\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\program files\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\program files\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\program files\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.2.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python312\\lib\\site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python312\\lib\\site-packages (from requests>=2.28->chromadb) (3.6)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from tokenizers>=0.13.2->chromadb) (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\program files\\python312\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\program files\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\program files\\python312\\lib\\site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (4.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\program files\\python312\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\narratic-dev002\\appdata\\roaming\\python\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc7ffb8-d806-4c54-a471-61ebd78e0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd227a-5409-4b89-91da-ba5c987d6fc7",
   "metadata": {},
   "source": [
    "# Preparing a RAG Pipeline Using Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2ebb4-f0de-4a10-9ba1-2e59dd98999e",
   "metadata": {},
   "source": [
    "Following the spirit of moving steadily but in small steps, we first set up a RAG pipeline with a sample text consisting of our class syllabus from the Canvas :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a9c707-bd99-4c8f-9b32-883ed72bf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cd499f-ca0c-4fea-87c3-a3ae2004084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Narratic-DEV002\\Desktop\\iat481-nlp-proj\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# checking which directory is our root\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229c44e-d296-4ac4-9aa0-6b831c6ca91a",
   "metadata": {},
   "source": [
    "Connecting ChromaDB is a trivial, [well-documented](https://python.langchain.com/docs/integrations/vectorstores/chroma/) task. We pick an appropriate file loader from LangChain's toolkit and split our text file into chunks. Splitting is an important step when working with external knowledge data since if we don't do it, we risk not fitting our augmented prompt into Llama-2's context window (4096 tokens max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e24605-4089-4466-abe6-b3eec6b9d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_chroma_db(db):\n",
    "    if db is None:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        db.delete_collection()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c963f5e-565b-45bc-96dc-fa9ae6178bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1394, which is longer than the specified 1024\n",
      "C:\\Users\\Narratic-DEV002\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Name: \n",
      "Exploring Artificial Intelligence: Its Use, Concepts, and Impact\n",
      "\n",
      "Course Description:\n",
      "\n",
      "This course is cross-listed as IAT481 (undergraduate) / IAT885 (graduate).\n",
      "\n",
      "This course is designed to provide a comprehensive and accessible introduction to the world of artificial intelligence that will empower the students to navigate the AI-driven future. Students will explore fundamental AI concepts, including machine learning, neural networks, natural language processing, and computer vision; discover real-world applications, ethical considerations, and the societal impact of AI. \n",
      "\n",
      " \n",
      "Course Info:\n",
      "\n",
      "Course will be held between Jan 8  Apr 12, 2024: Thu, 12:302:20 p.m. @ SRYC3170 . Tutorial sessions will be held weekly after the course @ SRYC3050\n",
      "\n",
      "Instructor: Dr. O. Nilay Yalcin oyalcin@sfu.ca , Office Hours: Wednesdays 12:30  2:30pm @SRYC 2282 (by email appointment only, contact at least 1 day before)\n",
      "\n",
      "TA: Maryiam Zahoor maryiam_zahoor@sfu.ca, Office Hours: Wednesdays SRYC 3120, 11am-12pm\n"
     ]
    }
   ],
   "source": [
    "# Code based on examples from the LangChain documentation: \n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "\n",
    "# load the sample document \n",
    "loader = TextLoader(\"./samples/481_syllabus.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# load it into Chroma\n",
    "try:\n",
    "    clear_chroma_db(db)  # Empty the database (this line is ignored if it's not been initialized yet)\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What the IAT481 course is about?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9329a-b05b-4ae5-a579-996f8516f604",
   "metadata": {},
   "source": [
    "Very nice! Querying the vector store indeed gave us a relevant chunk. Let's do some other query, though, to confirm this result is not random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "981e9cff-c9d5-4d4c-8e9a-ffab6fc4d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider e-mail etiquette,  http://www.albion.com/netiquette/corerules.html when sending an email to us. \n",
      "To promote understanding with your reader:\n",
      "\n",
      "    Write a clear subject line that shows your section number and the purpose of the email. Include course number in email subject: \"IAT 418/885: .... \". Thanks!\n",
      "\n",
      "    Identify your audience by name (i.e Hi Nilay or Hello Maryiam)\n",
      "\n",
      "    Compose a direct, concise message with a clear purpose (i.e I have a question about todays activity or I will not be in class next week.)\n",
      "\n",
      "    Proofread and use appropriate language for the context of your message--friendly and professional. \n",
      "\n",
      "    Close with your name and student number (i.e Regards, Brenda Sans (301001010))\n",
      "\n",
      "Email Protocols\n",
      "\n",
      "    Your Instructor and TA will reply to e-mails within 24 hours during weekdays.\n",
      "\n",
      "    We do not answer emails after 5pm, or on weekends and holidays.\n",
      "\n",
      "    Requests for grade changes and extensions must be sent directly to the course Instructor.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the e-mail policy in IAT 481?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836c4aa-7b30-497f-9b19-82d52cc27192",
   "metadata": {},
   "source": [
    "Looks good. The chunk returned to us matches our query. We can now plug this into Llama-2, replace the sample data with our Q&A pairs, and, hopefully, end up with a working RAG-powered application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2875d-fbad-4436-af2b-c25742135a1c",
   "metadata": {},
   "source": [
    "# Completing the RAG Pipeline Using Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ac598-dfd1-4a0c-bea4-0846af2dff4a",
   "metadata": {},
   "source": [
    "To recap, our project goal is get our chatbot to provide advice on better sleep. For a moment, we were not sure what data format to use for this purpose. However, some production-ready datasets such as [MedQuad-MedicalQnADataset by keivalya](https://huggingface.co/datasets/keivalya/MedQuad-MedicalQnADataset) are comprised of **simple Q&A pairs** listed in a normal **CSV file**. We chose the same route (thankfully, LangChain packages a file loader for that - [here is an example of its use](https://betterprogramming.pub/build-a-chatbot-on-your-csv-data-with-langchain-and-openai-ed121f85f0cd)).\n",
    "\n",
    "Basically, let's try loading and chunking our CSV data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb5f2311-d7fd-429e-964d-7b8e9d89c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90884185-c125-4f47-92ae-04f3ef3a62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./data/BetterSleep_QnADataset.csv\")\n",
    "\n",
    "csv_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d024083-ae03-46ba-85dc-c1f315836194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What sleep schedule is the most optimal?\n",
      "Answer: Research has shown it is better to set fixed times for going to bed and waking up and then adhere to them even on weekends. Our bodies heavily rely on circadian rhythms to regulate sleep patterns. If you go to bed at the same time every day it helps your body to acquire a steady habit of production of melatonin prior to your bedtime. Melatonin is a very important hormone that is related to circadian rhythms, day and night cycles. Increased level of melatonin is shown to help falling asleep faster and having a more quality sleep.\n"
     ]
    }
   ],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "data = text_splitter.split_documents(csv_data)\n",
    "\n",
    "# load it into Chroma\n",
    "clear_chroma_db(db)\n",
    "db = Chroma.from_documents(data, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What is the best sleep schedule?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f2283-c908-4d6d-9c2a-5d9fd6103065",
   "metadata": {},
   "source": [
    "Great, clearly our CSV data has been ingested into the vector DB successfully, and querying it behaves as expected. Our dataset is small, therefore a simple similarity search should suffice. It must be noted, however, that had we needed to pick top `k` chunks with even more precision and relevance (e.g. from a very large database with hundreds of thousands of records), this technique could be made more advanced by combining it with a **Reranker** - basically, a separate DNN model that reorders the results taking a deeper look into the nuances of semantics (an example description of that can be found [here](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)).\n",
    "\n",
    "The last missing puzzle piece is to modify our previous Llama-2 setup so that our prompt is augmented behind-the-scenes with data from our custom Q&A data. The beauty of LangChain is that there is a ready-made component for such a chain: `RetrievalQA`. We refer to the [official documentation](https://js.langchain.com/docs/modules/chains/popular/vector_db_qa) and, especially, [this article](https://www.mlexpert.io/blog/langchain-quickstart-with-llama-2#simple-retrieval-augmented-generation-rag) - both sources greatly helped us in setting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4494a028-6be8-425b-9a5c-d113f2271ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9b09a-09fe-41b9-bed8-b4437e0866ff",
   "metadata": {},
   "source": [
    "We also must re-write our prompt a bit (we partially sourced [this template](https://smith.langchain.com/hub/rlm/rag-prompt)), because now we are going to put in the prompt not only the question but also supplemented context (the core mechanism of RAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dfe3e0d-1928-4864-81e3-2d52e003ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"[INST]\n",
    "<<SYS>>\n",
    "You are helpful, respectful, caring and honest assistant for question-answering tasks. You do not have expressions or emotions. You are objective and provide everything that is helpful to know given the question, but you are not chatty, be concise and do not use more than three sentences. Use the following pieces of retrieved context to answer the question to the best of your ability. If you don't know the answer, just say that you don't know.\n",
    "<</SYS>>\n",
    "\n",
    "USER: {question}\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "ASSISTANT: \n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77cdd9-6130-4866-9ecd-10b9391d2aa0",
   "metadata": {},
   "source": [
    "Now we can prepare our RAG-enabling chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ab37c6a-99a7-4800-930b-6d7373e4497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "\n",
    "    # We use k=1 to always pick only the most relevant Q&A pair. Our dataset is small so that should suffice and we won't bloat the prompt\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe48749-64c0-4630-9e35-163e25ad0aa2",
   "metadata": {},
   "source": [
    "Oof, let's finally test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "080044cb-69ae-4a5e-b911-60072f57255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me some tips for the best sleep schedule\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a502492b-3de0-42c8-a7ab-4c4251d09a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Tell me some tips for the best sleep schedule\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Tell me some tips for the best sleep schedule\",\n",
      "  \"context\": \"Question: What sleep schedule is the most optimal?\\nAnswer: Research has shown it is better to set fixed times for going to bed and waking up and then adhere to them even on weekends. Our bodies heavily rely on circadian rhythms to regulate sleep patterns. If you go to bed at the same time every day it helps your body to acquire a steady habit of production of melatonin prior to your bedtime. Melatonin is a very important hormone that is related to circadian rhythms, day and night cycles. Increased level of melatonin is shown to help falling asleep faster and having a more quality sleep.\\n\\nQuestion: What are the phases of sleep?\\nAnswer: When you sleep, you cycle through two phases of sleep: rapid eye movement (REM) and non-REM sleep. The cycle starts over every 80 to 100 minutes. Usually there are four to six cycles per night. You may wake up briefly between cycles.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"[INST]\\n<<SYS>>\\nYou are helpful, respectful, caring and honest assistant for question-answering tasks. You do not have expressions or emotions. You are objective and provide everything that is helpful to know given the question, but you are not chatty, be concise and do not use more than three sentences. Use the following pieces of retrieved context to answer the question to the best of your ability. If you don't know the answer, just say that you don't know.\\n<</SYS>>\\n\\nUSER: Tell me some tips for the best sleep schedule\\n\\nCONTEXT: Question: What sleep schedule is the most optimal?\\nAnswer: Research has shown it is better to set fixed times for going to bed and waking up and then adhere to them even on weekends. Our bodies heavily rely on circadian rhythms to regulate sleep patterns. If you go to bed at the same time every day it helps your body to acquire a steady habit of production of melatonin prior to your bedtime. Melatonin is a very important hormone that is related to circadian rhythms, day and night cycles. Increased level of melatonin is shown to help falling asleep faster and having a more quality sleep.\\n\\nQuestion: What are the phases of sleep?\\nAnswer: When you sleep, you cycle through two phases of sleep: rapid eye movement (REM) and non-REM sleep. The cycle starts over every 80 to 100 minutes. Usually there are four to six cycles per night. You may wake up briefly between cycles.\\n\\nASSISTANT: \\n[/INST]\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get the best sleep schedule, it's important to set fixed times for going to bed and waking up and adhere to them consistently, even on weekends. This helps your body develop a steady habit of producing melatonin before bedtime, which can help you fall asleep faster and have a more restful night's sleep. Melatonin is a hormone that regulates circadian rhythms and is important for quality sleep.\n",
      "In terms of the phases of sleep, there are two main types: rapid eye movement (REM) and non-REM sleep. The cycle repeats every 80 to 100 minutes, with four to six cycles per night. You may briefly wake up between cycles."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4349.07 ms\n",
      "llama_print_timings:      sample time =      37.15 ms /   158 runs   (    0.24 ms per token,  4253.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22337.83 ms /   338 tokens (   66.09 ms per token,    15.13 tokens per second)\n",
      "llama_print_timings:        eval time =   17309.21 ms /   157 runs   (  110.25 ms per token,     9.07 tokens per second)\n",
      "llama_print_timings:       total time =   40409.61 ms /   495 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:LlamaCpp] [40.42s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To get the best sleep schedule, it's important to set fixed times for going to bed and waking up and adhere to them consistently, even on weekends. This helps your body develop a steady habit of producing melatonin before bedtime, which can help you fall asleep faster and have a more restful night's sleep. Melatonin is a hormone that regulates circadian rhythms and is important for quality sleep.\\nIn terms of the phases of sleep, there are two main types: rapid eye movement (REM) and non-REM sleep. The cycle repeats every 80 to 100 minutes, with four to six cycles per night. You may briefly wake up between cycles.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [40.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"To get the best sleep schedule, it's important to set fixed times for going to bed and waking up and adhere to them consistently, even on weekends. This helps your body develop a steady habit of producing melatonin before bedtime, which can help you fall asleep faster and have a more restful night's sleep. Melatonin is a hormone that regulates circadian rhythms and is important for quality sleep.\\nIn terms of the phases of sleep, there are two main types: rapid eye movement (REM) and non-REM sleep. The cycle repeats every 80 to 100 minutes, with four to six cycles per night. You may briefly wake up between cycles.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [40.43s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"To get the best sleep schedule, it's important to set fixed times for going to bed and waking up and adhere to them consistently, even on weekends. This helps your body develop a steady habit of producing melatonin before bedtime, which can help you fall asleep faster and have a more restful night's sleep. Melatonin is a hormone that regulates circadian rhythms and is important for quality sleep.\\nIn terms of the phases of sleep, there are two main types: rapid eye movement (REM) and non-REM sleep. The cycle repeats every 80 to 100 minutes, with four to six cycles per night. You may briefly wake up between cycles.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [40.46s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"To get the best sleep schedule, it's important to set fixed times for going to bed and waking up and adhere to them consistently, even on weekends. This helps your body develop a steady habit of producing melatonin before bedtime, which can help you fall asleep faster and have a more restful night's sleep. Melatonin is a hormone that regulates circadian rhythms and is important for quality sleep.\\nIn terms of the phases of sleep, there are two main types: rapid eye movement (REM) and non-REM sleep. The cycle repeats every 80 to 100 minutes, with four to six cycles per night. You may briefly wake up between cycles.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec1094-7db1-4f3a-b04e-cb9d6511c4ea",
   "metadata": {},
   "source": [
    "Simply awesome! The response we have got is sensible, concise and factually correct, and its generation did not take a lot of time (we get about ~10 tokens per second, which is just fine for streaming). We also can see in the debugging information that the relevant Q&A pair is being used as part of the hidden augmented prompt. Everything is in its right place and now we only need to package this code into a Python project and set it up so that it can be served to users through a simple web UI (any open-source chatbot UI would suffice, for example, [Gradio](https://www.gradio.app/) mentioned previously)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
